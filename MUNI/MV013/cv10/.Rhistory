hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(time), sd(time)), col='red')
lines(xx, dgamma(xx, 11, rate=0.27), col="blue")
qqnorm(time)
hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(time), sd(time)), col='red')
lines(xx, dgamma(xx, 11, rate=0.27), col="blue")
qqnorm(time)
qqline(time)
#normality_tests(time)
descdist(time, discrete = FALSE)
pois <- fitdist(time, "pois")
normm <- fitdist(time, "norm")
gamm <- fitdist(time, "gamma")
print("Poisson distribution")
plot(pois)
print("Normal distribution")
plot(normm)
print("Gamma distribution")
plot(gamm)
print("Based on the graphical visualisation in QQPlot, histogram, etc. the distribution function is most similar to
gamma distribution")
##H0 - Only 20% of all competitors qualified.
print("Null hyphothesis H0: Only 20% of all competitors qualified")
##H1 - More than 20% of all competitors qualified.
print("Alternative hyphothesis H1: More than 20% of all competitors qualified")
data <- read.csv("race.txt",sep=';')
data$time <- sort(data$time)
print("Random variable is finish time of competitors")
n1 <- length(data$year[data$year == 2020])
n2 <- length(data$year[data$year == 2021])
normality_tests(data$time)
n <- length(data$time)
xx <- 0:100
#Break sturges rule
num_breaks <- log2(n)+1
hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(data$time), sd(data$time)), col='red')
lines(xx, dgamma(xx, 11, rate=0.27), col="blue")
qqnorm(data$time)
qqline(data$time)
#normality_tests(data$time)
descdist(data$time, discrete = FALSE)
pois <- fitdist(data$time, "pois")
normm <- fitdist(data$time, "norm")
gamm <- fitdist(data$time, "gamma")
print("Poisson distribution")
plot(pois)
print("Normal distribution")
plot(normm)
print("Gamma distribution")
plot(gamm)
print("Based on the graphical visualisation in QQPlot, histogram, etc. the distribution function is most similar to
gamma distribution")
##H0 - Only 20% of all competitors qualified.
print("Null hyphothesis H0: Only 20% of all competitors qualified")
##H1 - More than 20% of all competitors qualified.
print("Alternative hyphothesis H1: More than 20% of all competitors qualified")
print("Based on the graphical visualisation in QQPlot, histogram, etc. the distribution function is most similar to
gamma distribution. To more fit normal distribution we would need to ignore times higher than 60.")
##H0 - Only 20% of all competitors qualified.
print("Null hyphothesis H0: Only 20% of all competitors qualified")
##H1 - More than 20% of all competitors qualified.
print("Alternative hyphothesis H1: More than 20% of all competitors qualified")
normality_tests(data$time)
#subtask a
normality_tests(data.length)
#Q-Q plot (step by step)
sample<-sort(data.length)
n<-length(data.length)
i<-1:n
beta<-0.5
alpha<-(i-beta)/(n+1-2*beta)
theoretical.quantiles<-qnorm(alpha,mean(data.length),sd(data.length))
plot(theoretical.quantiles,sample,main="Q-Q plot")
#Q-Q line
least_sqaure<-lm(sample~theoretical.quantiles)
xx<-seq(16,28,by=0.001)
lines(xx,least_sqaure$coefficients[1]+least_sqaure$coefficients[2]*xx)
#Q-Q plot ->function
qqnorm(data.length)
qqline(data.length)
hist(data.length)
hist(data.length, breaks=log2(length(data.length)))
hist(data.length, breaks=log2(length(data.length))+1)
#TASK 1
data.length<-c(19.92, 21.17, 24.87, 22.90, 20.88, 19.43, 24.39, 23.14, 16.99, 24.85, 20.80, 19.93,
22.28, 27.61, 27.50, 19.63, 20.35, 22.96, 17.57, 22.91)
#subtask a
normality_tests(data.length)
#Q-Q plot (step by step)
sample<-sort(data.length)
n<-length(data.length)
i<-1:n
beta<-0.5
alpha<-(i-beta)/(n+1-2*beta)
theoretical.quantiles<-qnorm(alpha,mean(data.length),sd(data.length))
plot(theoretical.quantiles,sample,main="Q-Q plot")
#Q-Q line
least_sqaure<-lm(sample~theoretical.quantiles)
xx<-seq(16,28,by=0.001)
lines(xx,least_sqaure$coefficients[1]+least_sqaure$coefficients[2]*xx)
#Q-Q plot ->function
qqnorm(data.length)
qqline(data.length)
hist(data.length, breaks=log2(length(data.length))+1)
#ECDF
plot(ecdf(data.length),main="Cumulative distribution function")
lines(xx,pnorm(xx,mean(data.length),sd(data.length)),col='red',lwd=3)
#P-P plot
cdf<-ecdf(data.length)
adj.empirical.cdf<-n/(n+1)*cdf(data.length)
theoretical.cdf<-pnorm(data.length,mean(data.length),sd(data.length))
plot(adj.empirical.cdf,theoretical.cdf,main="P-P plot")
#subtasks b-d
#H0: mu=20, H1: mu<>20
mu<-20
alpha<-0.05
x<-mean(data.length)
n<-length(data.length)
S<-sqrt(1/(n-1)*sum((data.length-x)^2))
T<-sqrt(n)*(x-mu)/S
#critical region
CR_1<-qt(alpha/2,n-1)
CR_2<-qt(1-alpha/2,n-1)
xx<-seq(-5,5,0.01)
plot(xx,dt(xx,n-1),type='l',main="Critical region",xlab="x",ylab = "f(x)",sub = paste("mu=",mu))
rect(-5,0,CR_1,1, col=rgb(red=1, green=0, blue=0, alpha=0.2), lty=0)
rect(CR_2,0,5,1, col=rgb(red=1, green=0, blue=0, alpha=0.2), lty=0)
abline(v=T,col='blue',lwd=2)
#p-value
p.value<-2*(1-pt(abs(T),n-1))
xx<-seq(-5,5,0.01)
plot(xx,dt(xx,n-1),type='l',main=paste("P-value",round(p.value,3)),xlab="x",ylab = "f(x)",sub = paste("mu=",mu))
rect(-5,0,-abs(T),1, col=rgb(red=0, green=0, blue=1, alpha=0.2), lty=0)
rect(abs(T),0,5,1, col=rgb(red=0, green=0, blue=1, alpha=0.2), lty=0)
abline(v=T,col='blue',lwd=2)
#TASK 2
data<-read.csv("Computers.csv")
price<-data[data$ram==2,"price"]
#subtask a
normality_tests(price)
qqnorm(price)
qqline(price)
plot(ecdf(price),main="Cumulative distribution function")
xx<-seq(900,2500,by=10)
lines(xx,pnorm(xx,mean(price),sd(price)),col='red',lwd=3)
par(mfrow=c(2,2))
h<-hist(price,xlim=c(700,2500),freq = F,ylim=c(0,0.0015))
#subtask a
normality_tests(data.length)
hist(data.length, breaks=10
hist(data.length, breaks=10)
hist(data.length, breaks=10)
hist(data.length, breaks=8)
hist(data.length, breaks=6)
qqnorm(data$time)
qqline(data$time)
qqnorm(data$time)
qqline(data$time)
###Problem 2
year2021 <- data[data$year == 2021]
###Problem 2
year2021 <- data$time[data$year == 2021]
###Problem 2
year2020 <- data$time[data$year == 2020]
boxplot(data$time)
boxplot(data$time~data$year)
help
sd
help(sd)
length(data$time)
boxplot(data$time~data$year)
data$time
setwd("~/school/MrPain_T/MUNI/MV013/HW3")
library(fitdistrplus)
library(fBasics)
normality_tests<-function(x){
#fBasics package
#https://www.rdocumentation.org/packages/fBasics/versions/240.10068.1/topics/OneSampleTests
#https://rdrr.io/cran/fBasics/src/R/test-normalityTest.R (implementation)
if (length(x)>20){ #at least 20 observation
print(dagoTest(x))
message('#####################')
}
print(jarqueberaTest(x))
message('#####################')
print(shapiroTest(x))
message('#####################')
if (length(unique(x))==length(x)){#ties should not be present for the Kolmogorov-Smirnov test
print(ks.test(data.length,"pnorm",mean(x),sd(x)))
}
message('#####################')
print(lillieTest(x))
message('#####################')
print(cvmTest(x))
message('#####################')
print(adTest(x))
message('#####################')
print(pchiTest(x)) #adhusted p-value
}
data <- read.csv("race.txt",sep=';')
data$time <- sort(data$time)
boxplot(data$time~data$year)
print("Random variable is finish time of competitors")
normality_tests(data$time)
n <- length(data$time)
xx <- 0:100
#Break sturges rule
num_breaks <- log2(n)+1
hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(data$time), sd(data$time)), col='red')
lines(xx, dgamma(xx, 11, rate=0.27), col="blue")
print("Based on the graphical visualisation in QQPlot and histogram we can say, that our data dont
follow normal distribution (are not normal).")
print("Based on the graphical visualisation in QQPlot and histogram we can say, that our data dont follow normal distribution (are not normal).")
qqnorm(data$time)
qqline(data$time)
help(circle)
help(circle)
circle()
hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(data$time), sd(data$time)), col='red')
lines(xx, dgamma(xx, 11, rate=0.27), col="blue")
ecdf(data$time)
ecdf(data$time).plot()
plot(ecdf(data$time))
lines(xx, dnorm(xx, mean(data$time), sd(data$time)))
lines(xx, pnorm(xx, mean(data$time), sd(data$time)))
plot(ecdf(data$time))
lines(xx, pnorm(xx, mean(data$time), sd(data$time)))
lines(xx, pnorm(xx, mean(data$time), sd(data$time)), col='red')
plot(ecdf(data$time))
lines(xx, pnorm(xx, mean(data$time), sd(data$time)), col='red')
plot(ecdf(data$time), main="Cumulative df")
lines(xx, pnorm(xx, mean(data$time), sd(data$time)), col='red')
qqnorm(data$time)
qqline(data$time)
plot(ecdf(data$time), main="Cumulative df")
lines(xx, pnorm(xx, mean(data$time), sd(data$time)), col='red')
qqline(data$time, col='red')
qqnorm(data$time)
qqline(data$time, col='red')
normality_tests(data$time)
adTest(data$time)
print(adTest(data$time))
print(pchiTest(data$time))
setwd("~/school/MrPain_T/MUNI/MV013/HW3")
library(fitdistrplus)
library(fBasics)
data <- read.csv("race.txt",sep=';')
data$time <- sort(data$time)
print("Random variable is finish time of competitors")
n <- length(data$time)
xx <- 0:100
#Break sturges rule
num_breaks <- log2(n)+1
hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(data$time), sd(data$time)), col='red')
lines(xx, dgamma(xx, 11, rate=0.27), col="blue")
hist(data$time, breaks = num_breaks, freq=FALSE)
#Normal distr
lines(xx, dnorm(xx, mean(data$time), sd(data$time)), col='red')
qqnorm(data$time)
qqline(data$time, col='red')
plot(ecdf(data$time), main="Cumulative df")
lines(xx, pnorm(xx, mean(data$time), sd(data$time)), col='red')
normality_tests(data$time)
print("Based on the graphical visualisation in CDF, QQPlot and histogram we can't say much so we use statistical tests.")
print(pchiTest(x))
print("Based on statistical test we can see that our p-value is higher than 0.05 so we can assume that our data come from normal distribution.")
print(pchiTest(data$time))
binomial(xx, 0.2)
help("binomial")
help(dbinom)
help(dbinom)
plot(xx, dbinom(xx, 80, mean(data$time)))
plot(xx, dbinom(xx, 80, mean(data$time)))
xx
plot(0:80, dbinom(0:80, 80, mean(data$time)))
plot(0:79, dbinom(0:79, 80, mean(data$time)))
plot(xx, dbinom(xx, 80, mean(data$time)))
help(dbinom)
dbinom(xx, 80, mean(data$time))
dbinom(xx, 80, 0.2)
plot(xx, dbinom(xx, 80, 0.2)
)
setwd("/home/pdancak/school/MrPain_T/MUNI/MV013/cv5")
ones_in_file <- function(fileName){
data<-readChar(fileName, file.info(fileName)$size)
data.int<-as.numeric(strsplit(data, "")[[1]])
len<-100
len.total<-length(data.int)
nones<-c()
for (i in seq(1,(len.total),len)){
nones <- c(sum(na.omit(data.int[i:(len+i-1)])), nones)
}
return(nones)}
zeros_in_file<-function(fileName){
data<-readChar(fileName, file.info(fileName)$size)
data.int<-as.numeric(strsplit(data, "")[[1]])
len<-100
len_total<-length(data.int)
nzeros<-c()
for (i in seq(1,(len_total),len)){
nzeros <- c(sum(data.int[i:(len+i)]),nzeros)
}
return(nzeros)}
#Create your function and then use optim function (see ?optim) to find coefficients of binomial distr.
#first file
fileName <- 'file.txt'
nzeros <- ones_in_file(fileName)
N = 100
fit_binom <- function(x, par){
return(sum(x)/(100*length(x)))
}
#par <- optim() #set up coefficients for optimization
hist(nzeros, xlim = c(0,100), freq = FALSE)
x <- 0:100
p_hat <- fit_binom(nzeros, 0)
lines(x, dbinom(x, 100, p_hat), col = 'red')
fit_binom_optim <- function(x, par){
return(-prod(dbinom(x,100,par)))
}
#second file
fileName <- 'file2.txt'
nzeros1 <- ones_in_file(fileName)
hist(nzeros1, xlim = c(0,100), freq = FALSE)
par1 <- optim(0.5, fit_binom, x=nzeros1, lower=0, upper=1, method="Brent") #set up coefficients for optimization
par2 <- optim(0.5, fit_binom_optim, x=nzeros1, lower=0, upper=1, method="Brent")
x <- 0:100
lines(x, dbinom(x, 100, par1$par[1]), col = 'red')
setwd("/home/pdancak/school/MrPain_T/MUNI/MV013/cv5")
ones_in_file <- function(fileName){
data<-readChar(fileName, file.info(fileName)$size)
data.int<-as.numeric(strsplit(data, "")[[1]])
len<-100
len.total<-length(data.int)
nones<-c()
for (i in seq(1,(len.total),len)){
nones <- c(sum(na.omit(data.int[i:(len+i-1)])), nones)
}
return(nones)}
zeros_in_file<-function(fileName){
data<-readChar(fileName, file.info(fileName)$size)
data.int<-as.numeric(strsplit(data, "")[[1]])
len<-100
len_total<-length(data.int)
nzeros<-c()
for (i in seq(1,(len_total),len)){
nzeros <- c(sum(data.int[i:(len+i)]),nzeros)
}
return(nzeros)}
#Create your function and then use optim function (see ?optim) to find coefficients of binomial distr.
#first file
fileName <- 'file.txt'
nzeros <- ones_in_file(fileName)
N = 100
fit_binom <- function(x, par){
return(sum(x)/(100*length(x)))
}
#par <- optim() #set up coefficients for optimization
hist(nzeros, xlim = c(0,100), freq = FALSE)
x <- 0:100
p_hat <- fit_binom(nzeros, 0)
lines(x, dbinom(x, 100, p_hat), col = 'red')
ones_in_file <- function(fileName){
data<-readChar(fileName, file.info(fileName)$size)
data.int<-as.numeric(strsplit(data, "")[[1]])
len<-100
len.total<-length(data.int)
nones<-c()
for (i in seq(1,(len.total-len+1),len)){
nones <- c(sum(data.int[i:(len+i-1)]), nones)
}
return(nones)}
#Create your function and then use optim function (see ?optim) to find coefficients of binomial distr.
#first file
fileName <- 'file.txt'
nzeros <- ones_in_file(fileName)
N = 100
fit_binom <- function(x, par){
return(sum(x)/(100*length(x)))
}
#par <- optim() #set up coefficients for optimization
hist(nzeros, xlim = c(0,100), freq = FALSE)
x <- 0:100
p_hat <- fit_binom(nzeros, 0)
lines(x, dbinom(x, 100, p_hat), col = 'red')
fit_binom_optim <- function(x, par){
return(-prod(dbinom(x,100,par)))
}
#second file
fileName <- 'file2.txt'
nzeros1 <- ones_in_file(fileName)
hist(nzeros1, xlim = c(0,100), freq = FALSE)
par1 <- optim(0.5, fit_binom, x=nzeros1, lower=0, upper=1, method="Brent") #set up coefficients for optimization
par2 <- optim(0.5, fit_binom_optim, x=nzeros1, lower=0, upper=1, method="Brent")
x <- 0:100
lines(x, dbinom(x, 100, par1$par[1]), col = 'red')
fit_normal <- function(x, par){
mu <- par[1]
sigma <- par[2]
-sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
#first file
par <- optim(c(50, 3), fit_normal, x=nzeros) #set up coefficients for optimization
hist(nzeros, xlim = c(0,100), freq = FALSE)
lines(0:100, dnorm(0:100, mean = par$par[1], sd = par$par[2]), col = 'red')
#second file
par <- optim(c(30, 5), fit_normal, x=nzeros1) #set up coefficients for optimization
#second file
par <- optim(c(30, 5), fit_normal, x=nzeros1) #set up coefficients for optimization
hist(nzeros1, xlim = c(0,100), freq = FALSE)
lines(0:100, dnorm(0:100, mean = par$par[1], sd = par$par[2]), col = 'red')
#second file
fileName <- 'file2.txt'
nzeros1 <- ones_in_file(fileName)
hist(nzeros1, xlim = c(0,100), freq = FALSE)
par1 <- optim(0.5, fit_binom, x=nzeros1, lower=0, upper=1, method="Brent") #set up coefficients for optimization
par2 <- optim(0.5, fit_binom_optim, x=nzeros1, lower=0, upper=1, method="Brent")
x <- 0:100
lines(x, dbinom(x, 100, par1$par[1]), col = 'red')
bin_petr <- function(par,x,n){
-sum(dbinom(x,n,par,log=T))
}
optim(0.5, bin_petr, x=nzeros, n=N, method="Brent", lower=0, upper=1)
optim(0.5, bin_petr, x=nzeros, n=N, method="Brent", lower=0, upper=1)$par
optim(0.5, bin_petr, x=nzeros, n=N, method="Brent", lower=0, upper=1)$par[1]
optim(0.5, bin_petr, x=nzeros, n=N, method="Brent", lower=0, upper=1)
#second file
fileName <- 'file2.txt'
nzeros1 <- ones_in_file(fileName)
hist(nzeros1, xlim = c(0,100), freq = FALSE)
par1 <- optim(0.5, fit_binom, x=nzeros1, lower=0, upper=1, method="Brent") #set up coefficients for optimization
par2 <- optim(0.5, fit_binom_optim, x=nzeros1, lower=0, upper=1, method="Brent")
x <- 0:100
lines(x, dbinom(x, 100, par1$par[1]), col = 'red')
neg_log_likeklihood = double(200)
neg_log_likeklihood = double(100)
par <- seq(0.01, 0.99, len=100)
length(x)
x <- 0:99
neg_log_likeklihood = double(100)
par <- seq(0.01, 0.99, len=100)
for(i in x) neg_log_likeklihood[i] = bin_petr(par[i], x=nzeros, n=N)
lines(par, neg_log_likeklihood, type="l")
plot(par, neg_log_likeklihood, type="l")
setwd("~/school/MrPain_T/MUNI/MV013/HW3")
setwd("~/school/MrPain_T/MUNI/MV013/HW3")
library(fBasics)
data <- read.csv("race.txt", sep=";")
attach(data)
time <- sort(time)
## Problem 2
data <- read.csv("race.txt", sep=";")
attach(data)
year20 <- time[year == 2020]
year21 <- time[year == 2021]
print("We need to add some times to make the vectors same length.
We use mean value of year21.")
year21 <- as.integer(c(year21, double(6)+mean(year21)))
print("We define differences of 2020-2021 times as Z.")
print("H0: u = 0 (mean of Z is = 0. The times are same.)")
print("H1: u < 0 (mean of Z is < 0. the times improved)")
print("We use left sided one-sample t-test")
alpha <- 0.05
Z <- year20 - year21
xx <- -60:60
plot(ecdf(Z),main="Cumulative distribution function")
lines(xx,pnorm(xx,mean(Z),sd(Z)),col='red',lwd=3)
hist(Z, freq = F, main="Histogram of 2020 - 2021")
lines(xx, dnorm(xx, mean(Z), sd(Z)), col="red")
setwd("/home/pdancak/school/MrPain_T/MUNI/MV013/cv11")
prices<-read.csv("Poiss", sep=" ")
prices
setwd("/home/pdancak/school/MrPain_T/MUNI/MV013/cv11")
data<-read.csv("Poiss", sep=" ")
setwd("/home/pdancak/school/MrPain_T/MUNI/MV013/cv10")
prices<-read.csv("GoldSilver.csv", sep=",")
prices$X <- as.Date(prices$X)
plot(prices$X, prices$gold, type="l")
lm()
setwd("/home/pdancak/school/MrPain_T/MUNI/MV013/cv10")
data<-read.csv("Computers.csv", sep=",")
head(data)
dim(data)
summary(data)
str(data)
#need to convert char to factor
data$cd <- as.factor(data$cd)
data$multi <- as.factor(data$multi)
data$premium <- as.factor(data$multi)
#check datatypes
str(data)
data$ram <- as.factor(data$ram)
data$screen <- as.factor(data$screen)
#b)
data$price <- as.numeric(data$price)
#check datatypes
str(data)
data$speed <- as.numeric(data$speed)
#b)
data$price <- as.numeric(data$price)
data$speed <- as.numeric(data$speed)
data$trend <- as.numeric(data$trend)
